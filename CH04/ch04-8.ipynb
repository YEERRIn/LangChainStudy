{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "     model_id=\"beomi/llama-2-ko-7b\",\n",
    "     task=\"text-generation\",\n",
    "     pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"beomi/llama-2-ko-7b\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)  \n",
    "pipe = pipeline(\"text-generation\", model=model,\n",
    "                tokenizer=tokenizer, max_new_tokens=512)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the following question in Korean.\n",
    "#Question: \n",
    "{question}\n",
    "\n",
    "#Answer: \"\"\" \n",
    "prompt = PromptTemplate.from_template(template) \n",
    "\n",
    "chain = prompt | hf | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\" \n",
    "\n",
    "print(\n",
    "    chain.invoke({\"question\": question})\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  \n",
    "    task=\"text-generation\",  \n",
    "    device=0,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 64},\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm  \n",
    "\n",
    "gpu_chain = prompt | gpu_llm | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"\n",
    "\n",
    "print(gpu_chain.invoke({\"question\": question}))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
